# Abstract

Deepfake technology poses significant threats to digital media integrity, necessitating robust and reliable detection systems. This project presents a comprehensive multi-method deepfake detection framework that combines perceptual dissimilarity mapping, temporal analysis, and ensemble fusion techniques to achieve superior detection accuracy. The system integrates four complementary detection approaches: plain frames classification using EfficientNet encoders for direct frame analysis, MRI-GAN-based perceptual artifact detection that highlights synthetic regions through perceptual dissimilarity maps, weighted ensemble fusion that adaptively combines multiple methods based on confidence scores, and temporal convolutional analysis using 1D CNNs to capture temporal inconsistencies across video sequences. Built upon the MRI-GAN architecture for generating perceptual dissimilarity maps, the framework employs advanced deep learning techniques including 1D convolutional neural networks to analyze frame sequences and detect temporal anomalies. The system supports rigorous K-fold cross-validation for model evaluation, probability calibration through temperature scaling for improved confidence estimates, and a production-ready web interface enabling real-time video analysis. Comprehensive evaluation demonstrates that the ensemble fusion approach achieves improved accuracy compared to individual methods, with enhanced robustness across diverse video conditions. The framework is designed for cross-platform deployment, supporting both GPU-accelerated and CPU-only environments, making it accessible for diverse computational resources and deployment scenarios. This integrated approach advances deepfake forensics toward trustworthy, maintainable, and scalable deployment in real-world applications.

**Keywords:** Deepfake detection, MRI-GAN, Temporal analysis, Ensemble fusion, Perceptual dissimilarity mapping, Video forensics

